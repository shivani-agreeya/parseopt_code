spark.hdfs.sequence.input = hdfs://10.20.40.150:8020/oneparser/data/dlf/
spark.hdfs.sequence.output = /oneparser/data/dmat/staging/
spark.hdfs.ftp.output = hdfs://10.20.40.150:8020/oneparser/data/dlf/
spark.hdfs.default.fs = hdfs://10.20.40.150:8020
#hbase.zookeeper.quorum = 10.20.40.150:2181,10.20.40.179:2181,10.20.50.15:2181
hbase.zookeeper.quorum = oneparserdnode01.vzwdt.local,oneparsernamenode.vzwdt.local,oneparsernamenode2.vzwdt.local
hbase.rootdir = file:///usr/hdp/2.6.5.0-292/hbase
zookeeper.znode.parent = /hbase-unsecure

spark.hdfs.hbase.sequence.input = hdfs://10.20.40.150:8020/oneparser/data/sequence/logviewer/

jdbc.driverClassName = org.postgresql.Driver
jdbc.url = jdbc:postgresql://10.20.40.116:5470/dmat_qa?currentSchema=dmat_qa
jdbc.username = postgres
jdbc.password = postgres123
prepost.report.url=https://vzwdt.com/DMATDEV/dmat/prepostrpt/v1/uploadprepostrpt?jobid=
server.port=8082
server.context-path=/PPR
spark.seqLimit = 10
spark.hiveLimit = 10
spark.esLimit = 10
spark.seqSleepTime = 15000
spark.hiveSleepTime = 15000
spark.esSleepTime = 15000
spark.timeInterval = 120
spark.seqTimeInterval = 120
spark.hiveTimeInterval = 120
spark.esTimeInterval = 120
spark.repartition = 1400
spark.CURRENT_ENVIRONMENT = DEV
spark.POSTGRES_CONN_URL = jdbc:postgresql://10.20.40.116:5470/dmat_qa?currentSchema=dmat_qa&user=postgres&password=postgres123
spark.POSTGRES_CONN_SDE_URL = jdbc:postgresql://10.20.40.116:5470/dmat_qa?currentSchema=sde&user=postgres&password=postgres123&socketTimeout=5
spark.POSTGRES_DRIVER = org.postgresql.Driver
spark.POSTGRES_DB_URL = jdbc:postgresql://10.20.40.116:5470/dmat_qa?currentSchema=dmat_qa
spark.POSTGRES_DB_SDE_URL = jdbc:postgresql://10.20.40.116:5470/dmat_qa?currentSchema=sde
spark.POSTGRES_DB_USER = postgres
spark.POSTGRES_DB_PWD = postgres123
spark.HIVE_HDFS_PATH = hdfs://10.20.40.150:8020/apps/hive/warehouse/dmat_logs.db/
spark.HDFS_FILE_DIR_PATH = hdfs://10.20.40.150:8020/oneparser/data/dmat/staging/
spark.HDFS_URI = hdfs://10.20.40.150:8020
spark.HIVE_DB = dmat_logs.
spark.MaxRTP_PacketLossVal = 10000
spark.TBL_QCOMM_NAME = LogRecord_QComm_Ext
spark.TBL_QCOMM2_NAME = LogRecord_QComm2_Ext
spark.TBL_B192_NAME = LogRecord_B192_Ext
spark.TBL_B193_NAME = LogRecord_B193_Ext
spark.TBL_ASN_NAME = LogRecord_ASN_Ext
spark.TBL_NAS_NAME = LogRecord_NAS_Ext
spark.TBL_IP_NAME = LogRecord_IP_Ext
spark.TBL_QCOMM5G_NAME = LogRecord_QComm_5G_Ext
spark.ES_HOST_PATH = 10.20.40.28
spark.ES_PORT_NUM = 9201
spark.DM_USER_DUMMY = 10088
spark.REPORT_LOG_CUTOFF_DATE = 2019-09-19

spark.SPARK_MASTER = local[2]
spark.APP_NAME = SingleParserProcess
spark.DRIVER_BIND_ADDRESS = 127.0.0.1
spark.DEBUG_MAX_TO_STRING_FIELDS = 300

spark.ZOOKEEPER_HOSTS = 127.0.0.1:2181
spark.BOOTSTRAP_SERVERS = 127.0.0.1:9092
spark.MAX_OFFSETS_PER_TRIGGER = 5
spark.STARTING_OFFSETS = earliest
spark.KAFKA_GROUP_ID = DMAT
spark.INGESTION_KAFKA_TOPIC = logs
spark.NOTIFICATION_KAFKA_TOPIC = logsnotification
spark.PROCESS_PARAM_VALUES_KAFKA_TOPIC = processparamvalues
spark.HIVE_KAFKA_TOPIC = logshive
spark.ES_KAFKA_TOPIC = logses
spark.SCHEMA_REGISTRY_URL = http://127.0.0.1:8081
spark.PARAM_VALUE_SCHEMA_REGISTRY_ID = latest
spark.CHECKPOINT_LOCATION = checkpoints
