spark.CURRENT_ENVIRONMENT DEV
spark.POSTGRES_CONN_URL jdbc:postgresql://localhost:5432/postgres?currentSchema=dmat_qa&user=postgres&password=postgres-d
spark.POSTGRES_CONN_SDE_URL jdbc:postgresql://localhost:5432/postgres?currentSchema=sde&user=postgres&password=postgres-d&socketTimeout=5
spark.POSTGRES_DRIVER org.postgresql.Driver
spark.POSTGRES_DB_URL jdbc:postgresql://localhost:5432/postgres?currentSchema=dmat_qa
spark.POSTGRES_DB_USER postgres
spark.POSTGRES_DB_PWD postgres-d
spark.HIVE_HDFS_PATH hdfs://namenode:9000/apps/hive/warehouse/dmat_logs.db/
spark.HDFS_FILE_DIR_PATH hdfs://namenode:9000/oneparser/data/dmat/staging/
spark.HDFS_URI hdfs://localhost:8020
spark.WRITE_HDFS_URI hdfs://localhost:9000
spark.DM_USER_DUMMY 10088
spark.HIVE_DB dmat_logs.
spark.FINAL_HIVE_TBL_NAME FinalLogRecord_DMAT
spark.HIVE_MIN_PARTITION 50
spark.LOG_GENERAL_EXCEPTION FALSE
spark.LOG_LEVEL WARN
spark.ZOOKEEPER_HOSTS localhost:2181
spark.BOOTSTRAP_SERVERS localhost:9092
spark.MAX_OFFSETS_PER_TRIGGER 25
spark.STARTING_OFFSETS earliest
spark.KAFKA_GROUP_ID DMAT
spark.INGESTION_KAFKA_TOPIC logs
spark.NOTIFICATION_KAFKA_TOPIC ns
spark.PROCESS_PARAM_VALUES_KAFKA_TOPIC process
spark.HIVE_KAFKA_TOPIC hiveprocessor
spark.ES_KAFKA_TOPIC elastic
spark.SCHEMA_REGISTRY_URL http://localhost:8081
spark.PARAM_VALUE_SCHEMA_REGISTRY_ID latest
spark.CHECKPOINT_LOCATION file:////checkpoints
spark.WRITE_SEQ_TO_HDFS_CHECKPOINT_SEGMENT write_seq_to_hdfs
spark.READ_SEQ_TO_HIVE_CHECKPOINT_SEGMENT read_seq_to_hive
spark.PROCESS_DMAT_RECORDS_CHECKPOINT_SEGMENT process_dmat_records

spark.ES_HOST_PATH localhost
spark.ES_PORT_NUM 9200
spark.TBL_QCOMM_NAME LogRecord_QComm_Ext_D_05052020
spark.TBL_QCOMM2_NAME LogRecord_QComm2_Ext_D_05052020
spark.TBL_B192_NAME LogRecord_B192_Ext_D_05052020
spark.TBL_B193_NAME LogRecord_B193_Ext_D_05052020
spark.TBL_ASN_NAME LogRecord_ASN_Ext_D_05052020
spark.TBL_NAS_NAME LogRecord_NAS_Ext_D_05052020
spark.TBL_IP_NAME LogRecord_IP_Ext_D_05052020
spark.TBL_QCOMM5G_NAME LogRecord_QComm_5G_Ext_D_05052020

spark.SEQ_FILE_DEL FALSE
spark.DETAILED_HIVE_JOB_ANALYSIS FALSE
spark.REPORTS_HIVE_TBL_NAME _D_05052020
spark.PROCESSING_SERVER DEV
spark.PROCESS_RM_ONLY FALSE
spark.ES_CLUSTER_NAME dmat_es
spark.ES_NODE_ID localhost
spark.ES_PORT_ID 9301
spark.ES_CELLSITE_INDEX cellsite
spark.PROCESS_FILE_TYPE ALL
spark.es_cluster_name dmat_es

hive.metastore.client.connect.retry.delay 5
hive.metastore.client.socket.timeout 1800
hive.metastore.uris thrift://localhost:9083
hive.server2.enable.doAs false
hive.server2.thrift.port 10016
hive.server2.transport.mode binary

spark.repartition 8
spark.sql.shuffle.partitions 16
spark.writeToHdfsRepartition 8
spark.PROCESSDMAT_HDFS_WRITE false
spark.cellsite_weekly_index_date 2018-07-01
spark.es_node_id localhost
spark.es_port_id 9300
spark.es_cellsite_index cellsite
spark.ES_INDEX_PREFIX
spark.KPI_RTP_PACKET true
spark.KPI_RTP_PACKET_BROADCAST false
spark.KPI_RTP_PACKET_CACHE false
spark.GEO_SHAPE_FILE_PATH /oneparser/data/dmat/geoShapes/
spark.USE_GEO_SHAPE_FILES true
spark.email_host_name
spark.email_port 25
spark.email_user_name
spark.email_pwd
spark.email_from
spark.email_sender_name
spark.sql.broadcastTimeout 1200s
spark.network.timeout 6000s
spark.storage.blockManagerSlaveTimeoutMs 6000s
spark.executor.heartbeatInterval 600s
spark.sql.adaptive.enabled true

spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryo.classesToRegister org.apache.spark.sql.execution.joins.UnsafeHashedRelation,org.apache.spark.sql.execution.datasources.WriteTaskResult,org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage,org.apache.spark.sql.execution.datasources.ExecutedWriteSummary,org.apache.spark.sql.execution.datasources.BasicWriteTaskStats,[Lorg.apache.spark.sql.catalyst.InternalRow;,scala.reflect.ClassTag$$anon$1,org.apache.spark.sql.execution.columnar.CachedBatch
spark.kryo.registrationRequired true

spark.sql.optimizer.dynamicPartitionPruning.enabled false
spark.hadoop.hive.metastore.uris thrift://localhost:9083

spark.PATH_TO_CORE_SITE file:////HadoopConf/core-site.xml
spark.PATH_TO_HDFS_SITE file:////HadoopConf/hdfs-site.xml